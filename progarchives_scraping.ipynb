{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "419fada4",
   "metadata": {},
   "source": [
    "# ProgArchives Web scraping\n",
    "\n",
    "In this code, we perform web scraping of a website. Web scraping is the activity of extracting information \n",
    "from one or more websites in an automated way. We usually do this when we want data that is not available \n",
    "through APIs.\n",
    "\n",
    "The target site is [Prog archives](http://www.progarchives.com/). It is a progressive rock website with an \n",
    "active community that indexes various genres, bands and albums, with ratings and comments. This website is \n",
    "the most complete and powerful progressive rock resource. Prog Archives has a very robust search engine that \n",
    "allows us to search the albums of each year, by genre and country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35621324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some important modules to scrape a website.\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import sys\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d95612d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTTP headers let the client and the server pass additional information with an HTTP request or response.\n",
    "headers = {'User-Agent': \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:108.0) Gecko/20100101 Firefox/108.0\"}\n",
    "\n",
    "# Perform a GET type request and check if the response had a status code 200.\n",
    "page = requests.get('http://www.progarchives.com/top-prog-albums.asp?salbumtypes=1&smaxresults=250#list', headers=headers)\n",
    "\n",
    "if page.status_code != 200:\n",
    "    sys.exit('Non 200 status code received')\n",
    "\n",
    "# Parse the html into the 'page' variable and store it in Beautiful Soup format.\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "# Table that organizes the top rating albuns.\n",
    "table = soup.find('table', attrs={'cellpadding':'7'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a89259ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag that contains all the album information.\n",
    "table_tag = table.find_all('tr')\n",
    "\n",
    "artist = [table_tag[i].find_all('a')[1].get_text() for i in range(0, len(table_tag))]\n",
    "\n",
    "album = [table_tag[i].find_all('a')[0].get_text() for i in range(0, len(table_tag))]\n",
    "\n",
    "rating = [table_tag[i].find_all('td')[2].find('span').get_text() for i in range(0, len(table_tag))]\n",
    "\n",
    "QWR = [table_tag[i].find_all('td')[2].find('div', attrs={'style':\"font-size:80%;\"}).get_text() for i in range(0, len(table_tag))]\n",
    "\n",
    "genre = [table_tag[i].find_all('strong')[2].get_text() for i in range(0, len(table_tag))]\n",
    "\n",
    "year = [table_tag[i].find_all('br')[1].text for i in range(0, len(table_tag))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea5181b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset in csv file.\n",
    "colums_names = ['artist', 'album', 'rating', 'QWR', 'genre', 'year']\n",
    "with open('data/raw_data/raw_top_250_progarchives.csv', 'w', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f, delimiter=',')\n",
    "    writer.writerow(colums_names)\n",
    "    writer.writerows(zip(artist, album, rating, QWR, genre, year))\n",
    "    \n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
